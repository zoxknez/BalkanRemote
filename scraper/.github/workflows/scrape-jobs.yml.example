name: Daily Job Scraper

on:
  # Run daily at 6 AM UTC (8 AM Belgrade time)
  schedule:
    - cron: '0 6 * * *'
  
  # Allow manual trigger from GitHub UI
  workflow_dispatch:
    inputs:
      limit:
        description: 'Limit jobs per source'
        required: false
        default: '200'
      mode:
        description: 'Scraping mode (remote, balkan, or all)'
        required: false
        default: 'remote'

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          cd scraper
          pip install -r requirements.txt
      
      - name: Run scraper
        run: |
          cd scraper
          
          # Use workflow inputs or defaults
          LIMIT="${{ github.event.inputs.limit || '200' }}"
          MODE="${{ github.event.inputs.mode || 'remote' }}"
          
          echo "[INFO] Running scraper with limit=$LIMIT, mode=$MODE"
          
          if [ "$MODE" = "remote" ]; then
            python scrape.py --remote --limit $LIMIT
          elif [ "$MODE" = "balkan" ]; then
            python scrape.py --balkan --limit $LIMIT
          else
            python scrape.py --limit $LIMIT
          fi
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      
      - name: Load to Supabase
        run: |
          cd scraper
          python scrape.py --load
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      
      - name: Verify database
        run: |
          cd scraper
          python scrape.py --check
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      
      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: scraped-jobs-${{ github.run_number }}
          path: scraper/out/jobs.ndjson
          retention-days: 7
      
      - name: Notify on failure
        if: failure()
        run: |
          echo "[ERROR] Scraper failed! Check logs above."
          exit 1

# Secrets needed in GitHub repo settings:
# - SUPABASE_URL
# - SUPABASE_SERVICE_ROLE_KEY

