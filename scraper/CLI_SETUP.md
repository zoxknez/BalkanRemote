# CLI Setup za Automated Deployment

Ako Å¾eliÅ¡ da setup-ujem automatizovano scraping sa Vercel Cron ili GitHub Actions, evo Å¡ta mi treba:

## ğŸ” Credentials Needed

### 1. Vercel CLI Access (Opciono)

```bash
# NaÄin 1: Login lokalno (ti radiÅ¡)
npm i -g vercel
vercel login
vercel link

# NaÄin 2: Access Token (daÅ¡ mi)
# Settings â†’ Tokens â†’ Create Token
VERCEL_TOKEN=your_token_here
```

**Å ta mogu sa ovim:**
- Deploy scraper kao Vercel Function
- Setup Vercel Cron (free plan allows 1 cron job)
- Auto-scrape svaki dan u odreÄ‘eno vreme

---

### 2. Supabase CLI Access (Opciono)

```bash
# NaÄin 1: Login lokalno (ti radiÅ¡)
npm i -g supabase
supabase login
supabase link --project-ref your-ref

# NaÄin 2: Service Role Key (veÄ‡ imaÅ¡)
# Settings â†’ API â†’ service_role key
SUPABASE_SERVICE_ROLE_KEY=eyJhbGc...
```

**Å ta mogu sa ovim:**
- Run migrations
- Setup database functions
- Manage tables

---

## ğŸš€ Deployment Options

### Option A: Vercel Cron (PreporuÄeno za Auto-Scraping)

**Setup:**
1. Kreiram API route: `/api/scrape-jobs`
2. Konfiguriram `vercel.json` sa cron schedulom
3. Deploy sa `vercel --prod`

**vercel.json:**
```json
{
  "crons": [{
    "path": "/api/scrape-jobs",
    "schedule": "0 6 * * *"
  }]
}
```

**Rezultat:**
- âœ… Auto-scrape svaki dan u 6 AM
- âœ… Direktno load u Supabase
- âœ… Free na Hobby plan (1 cron job)

**Å ta mi treba:**
- Vercel login ili token
- Potvrda da Å¾eliÅ¡ ovu opciju

---

### Option B: GitHub Actions (Alternative)

**Setup:**
1. Kreiram `.github/workflows/scrape-jobs.yml`
2. Dodajem secrets u GitHub repo
3. Auto-run na schedule

**scrape-jobs.yml:**
```yaml
name: Scrape Jobs
on:
  schedule:
    - cron: '0 6 * * *'  # Daily at 6 AM UTC
  workflow_dispatch:  # Manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - run: pip install -r scraper/requirements.txt
      - run: cd scraper && python scrape.py --remote --limit 200 && python scrape.py --load
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
```

**Rezultat:**
- âœ… Auto-scrape svaki dan
- âœ… Free (GitHub Actions 2000 min/month)
- âœ… Easy monitoring u Actions tab

**Å ta mi treba:**
- GitHub repo access (push kod)
- Dodaj secrets u repo settings

---

### Option C: Railway.app / Render.com Cron

**Setup:**
1. Kreiram `cron-job.sh` script
2. Deploy kao Cron Job na Railway/Render
3. Auto-run daily

**Rezultat:**
- âœ… Dedicated cron runner
- âš ï¸ Paid ($5-10/month)

---

## ğŸ¯ Moja Preporuka

**Za tvoj use case (manualno + opciono auto):**

### Setup 1: GitHub Actions (Najbolje!)

**Pros:**
- âœ… Free
- âœ… Easy setup
- âœ… Dobar monitoring
- âœ… MoÅ¾e i manual trigger
- âœ… Ne traÅ¾i Vercel Pro

**Cons:**
- Nema (sve je dobro!)

**Setup koraci:**
1. Dam ti `.github/workflows/scrape-jobs.yml` fajl
2. Ti push-ujeÅ¡ na GitHub
3. DodaÅ¡ secrets u repo settings:
   - `SUPABASE_URL`
   - `SUPABASE_SERVICE_ROLE_KEY`
4. âœ… Done! Auto-scrape svaki dan + manual kada god hoÄ‡eÅ¡

---

### Setup 2: Samo Manualno (Trenutno)

**Pros:**
- âœ… VeÄ‡ radi!
- âœ… Puna kontrola
- âœ… Nema troÅ¡kova

**Cons:**
- MoraÅ¡ ruÄno pokretati

**Kako radiÅ¡:**
```bash
# Kada god hoÄ‡eÅ¡ fresh jobs
cd scraper
python scrape.py --remote --limit 200 && python scrape.py --load
```

---

## ğŸ¤” Å ta biraÅ¡?

**Opcija 1: GitHub Actions (Auto + Manual)**
â†’ DaÄ‡u ti workflow fajl, ti push-ujeÅ¡ i dodaÅ¡ secrets

**Opcija 2: Manualno (Trenutno reÅ¡enje)**  
â†’ NiÅ¡ta se ne menja, pokreÄ‡eÅ¡ kad hoÄ‡eÅ¡

**Opcija 3: Vercel Cron**  
â†’ Treba mi login ili token

**Opcija 4: Railway/Render**  
â†’ Setup za paid plan

---

## ğŸ“ SledeÄ‡i Koraci (ako biraÅ¡ GitHub Actions)

1. Kreiram `.github/workflows/scrape-jobs.yml`
2. Commit + push na tvoj repo
3. IdeÅ¡ u GitHub:
   - Settings â†’ Secrets â†’ New repository secret
   - DodaÅ¡ `SUPABASE_URL`
   - DodaÅ¡ `SUPABASE_SERVICE_ROLE_KEY`
4. Test manual trigger:
   - Actions â†’ Scrape Jobs â†’ Run workflow
5. âœ… Radi! Od sutra auto-scrape svaki dan u 6 AM

**HoÄ‡eÅ¡ ovo?** Javi mi pa zavrÅ¡im setup! ğŸš€

---

## ğŸ”§ Already Working

- âœ… Scraper system perfektno radi
- âœ… Load u Supabase radi
- âœ… 1,821 poslova u bazi
- âœ… Komande jednostavne: `python scrape.py --remote --limit 200`

**Sada odluÄi:** Auto ili Manual? ğŸ˜Š

