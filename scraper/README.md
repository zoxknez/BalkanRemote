# Balkan Jobs Scraper ğŸ•·ï¸

Python scraper za prikupljanje oglasa za poslove sa Balkan izvora (Infostud, Halo Oglasi, MojPosao, Posao.ba, MojeDelo) i internacionalnih remote job board-ova (RemoteOK, WeWorkRemotely, itd.).

## Features

- âœ… **Crawlee framework** - robustan scraping sa anti-bot zaÅ¡titom
- âœ… **Playwright** - headless browser za scraping JavaScript sajtova
- âœ… **Smart filtering** - samo oglasi iz zadnjih 24-48h
- âœ… **Deduplication** - provera da oglas veÄ‡ ne postoji u bazi
- âœ… **Quality scoring** - automatska ocena kvaliteta podataka
- âœ… **Supabase integration** - automatski upsert u produkcijsku bazu
- âœ… **Dry run mode** - testiranje bez pisanja u bazu

## Struktura

```
scraper/
â”œâ”€â”€ scrapers/          # Scrapers za pojedinaÄne izvore
â”‚   â”œâ”€â”€ infostud.py    # Poslovi Infostud (Srbija)
â”‚   â”œâ”€â”€ remoteok.py    # RemoteOK (Remote)
â”‚   â””â”€â”€ ...            # (TODO: dodati ostale)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ database.py    # Supabase upsert logika
â”‚   â”œâ”€â”€ normalizer.py  # Normalizacija podataka
â”‚   â””â”€â”€ logger.py      # Logger
â”œâ”€â”€ main.py            # Main orchestrator
â”œâ”€â”€ requirements.txt   # Python dependencies
â””â”€â”€ .env               # Environment variables
```

## Setup

### 1. Install Python dependencies

```bash
cd scraper
pip install -r requirements.txt
```

**Potreban je Python 3.11+**

### 2. Install Playwright browsers

```bash
playwright install chromium
```

### 3. Create `.env` file

```bash
cp .env.example .env
```

Popuni `.env` sa svojim credentials:

```env
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_ROLE_KEY=your_service_role_key

MAX_JOBS_PER_SOURCE=50
SCRAPE_HOURS_BACK=24
DRY_RUN=false
```

## Usage

### Run scraper

```bash
# Production mode (piÅ¡e u bazu)
python main.py

# Dry run mode (samo testira, ne piÅ¡e u bazu)
DRY_RUN=true python main.py

# OgraniÄi broj oglasa po izvoru
MAX_JOBS_PER_SOURCE=20 python main.py
```

### Output

```
============================================================
ğŸš€ Balkan Jobs Scraper Started
============================================================
Max jobs per source: 50
Dry run mode: false

ğŸ“ Scraping HYBRID/ONSITE jobs...

ğŸ‡·ğŸ‡¸ Scraping Infostud...
[12:34:56] â„¹ï¸  Starting Infostud scraper (max: 50 jobs)
[12:34:58] â„¹ï¸  Scraped: Senior Frontend Developer at Tech Srbija
[12:34:59] â„¹ï¸  Scraped: Backend Engineer at Nordeus
...
[12:35:10] âœ… Infostud scraper completed: 25 jobs

ğŸŒ Scraping REMOTE jobs...

ğŸŒ Scraping RemoteOK...
[12:35:12] â„¹ï¸  Starting RemoteOK scraper (max: 50 jobs)
[12:35:15] â„¹ï¸  Scraped: Full Stack Developer at GitLab
...
[12:35:30] âœ… RemoteOK scraper completed: 40 jobs

============================================================
ğŸ“Š SUMMARY
============================================================
Total HYBRID jobs scraped: 25
Total REMOTE jobs scraped: 40

ğŸ’¾ Upserting to database...
[12:35:32] âœ… Hybrid jobs: 18 inserted, 7 skipped
[12:35:34] âœ… Remote jobs: 35 inserted, 5 skipped

============================================================
âœ… Scraper completed in 45.23s
============================================================
```

## Data Schema

### Hybrid/Onsite jobs â†’ `hybrid_jobs` table

```python
{
    'external_id': 'infostud-12345',
    'source_name': 'Poslovi Infostud',
    'title': 'Senior Frontend Developer',
    'company_name': 'Tech Srbija',
    'location': 'Beograd, Srbija',
    'work_type': 'hybrid',
    'country_code': 'RS',
    'region': 'BALKAN',
    'category': 'software-engineering',
    'description': '...',
    'application_url': 'https://...',
    'experience_level': 'senior',
    'employment_type': 'full-time',
    'salary_min': 2000,
    'salary_max': 3500,
    'salary_currency': 'EUR',
    'skills': ['React', 'TypeScript', 'Next.js'],
    'posted_date': '2025-10-05T12:00:00',
    'scraped_at': '2025-10-06T12:35:10',
    'quality_score': 85
}
```

### Remote jobs â†’ `jobs` table

```python
{
    'external_id': 'remoteok-abc123',
    'source_name': 'RemoteOK',
    'title': 'Full Stack Developer',
    'company_name': 'GitLab',
    'location': 'Remote',
    'work_type': 'remote',
    'country_code': None,
    'region': 'GLOBAL',
    'category': 'software-engineering',
    # ... ostali podaci sliÄno kao gore
}
```

## Adding New Scrapers

### 1. Create new scraper file

```python
# scrapers/mojposao.py
from crawlee.playwright_crawler import (
    PlaywrightCrawler,
    PlaywrightCrawlingContext
)
from utils.logger import Logger

logger = Logger("mojposao")

async def scrape_mojposao(max_jobs: int = 50):
    jobs = []
    
    async def request_handler(context: PlaywrightCrawlingContext):
        # Your scraping logic here
        pass
    
    crawler = PlaywrightCrawler(
        request_handler=request_handler,
        headless=True
    )
    
    await crawler.run(["https://www.mojposao.net/"])
    
    return jobs
```

### 2. Add to main.py

```python
from scrapers.mojposao import scrape_mojposao

# In main():
try:
    logger.info("ğŸ‡­ğŸ‡· Scraping MojPosao...")
    mojposao_jobs = await scrape_mojposao(max_jobs)
    all_jobs['hybrid'].extend(mojposao_jobs)
    logger.success(f"MojPosao: {len(mojposao_jobs)} jobs scraped")
except Exception as e:
    logger.error("MojPosao scraper failed", e)
```

## Scheduling

### Option 1: Manual run (lokalno)

Pokreni svaki dan ruÄno ili sa cron job-om:

```bash
# Windows Task Scheduler
# Linux/Mac crontab:
0 6 * * * cd /path/to/scraper && python main.py
```

### Option 2: Automated (GitHub Actions, Render, Railway)

MoÅ¾emo dodati nakon testiranja scrapers-a.

## Implemented Scrapers

### âœ… Hybrid/Onsite Jobs (Balkan) - 5 scrapers
- âœ… **Infostud** (Srbija) - https://www.poslovi.infostud.com
- âœ… **Halo Oglasi** (Srbija) - https://www.halooglasi.rs
- âœ… **MojPosao** (Hrvatska) - https://www.mojposao.net
- âœ… **Posao.ba** (BiH) - https://www.posao.ba
- âœ… **MojeDelo** (Slovenija) - https://www.mojedelo.com

### âœ… Remote Jobs (Global) - 8 scrapers
- âœ… **RemoteOK** - https://remoteok.com
- âœ… **WeWorkRemotely** - https://weworkremotely.com
- âœ… **Remotive** - https://remotive.io
- âœ… **JustRemote** - https://justremote.co
- âœ… **Remote.co** - https://remote.co
- âœ… **Working Nomads** - https://www.workingnomads.com
- âœ… **Remote.io** - https://remote.io
- âœ… **Himalayas** - https://himalayas.app

**Total: 13 scrapers** covering all major Balkan job boards + top 8 global remote job sites!

## Expected Daily Output

With default settings:
- **Hybrid/Onsite**: 5 scrapers Ã— 100 jobs = **~500 jobs/day** âœ…
- **Remote**: 8 scrapers Ã— 200 jobs = **~1,600 jobs/day** âœ…
- **TOTAL**: **~2,100 jobs/day** ğŸ¯

Limits can be adjusted in `.env`:
- `MAX_JOBS_PER_SOURCE_REMOTE=200` (for remote scrapers)
- `MAX_JOBS_PER_SOURCE_HYBRID=100` (for hybrid scrapers)

## TODO

- [ ] Dodati date filtering (samo oglasi iz zadnjih 24h)
- [ ] PoboljÅ¡ati quality scoring
- [ ] Dodati error recovery i retry logic
- [ ] Unit tests
- [ ] Dodati viÅ¡e remote izvora (Remote.co, FlexJobs, Remote.com, itd.)

## Notes

- Scraper se pokreÄ‡e **lokalno** (ne ide na GitHub)
- Dodati `scraper/` u `.gitignore` (opciono)
- Svaki scraper moÅ¾e imati razliÄitu strukturu HTML-a
- Crawlee automatski handluje proxy rotation i anti-bot zaÅ¡titu
- Upsert u Supabase radi na `external_id` (deduplikacija)

---

**Autor:** RemoteBalkan Team  
**Verzija:** 1.0.0  
**Datum:** Oktobar 2025
